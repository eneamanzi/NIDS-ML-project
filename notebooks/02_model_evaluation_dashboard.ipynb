{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a743ed02",
   "metadata": {},
   "source": [
    "# üõ°Ô∏è NIDS Model Evaluation Dashboard\n",
    "\n",
    "Dashboard interattiva per valutare, confrontare e spiegare i modelli di Network Intrusion Detection.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bda18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import os\n",
    "import glob\n",
    "import shap\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc3d519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURAZIONE PATH\n",
    "# Se il notebook √® in 'notebooks/', torniamo indietro di uno per trovare 'data' e 'models'\n",
    "BASE_DIR = '../'\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'data/processed/CICIOT23')\n",
    "MODELS_DIR = os.path.join(BASE_DIR, 'models')\n",
    "\n",
    "print(\"üìÇ Directory configurate:\")\n",
    "print(f\"   Data: {DATA_DIR}\")\n",
    "print(f\"   Models: {MODELS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed065ea2",
   "metadata": {},
   "source": [
    "## 1. Caricamento Dati (Test Set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c73b7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES = 50000 # Riduci se la RAM √® piena\n",
    "\n",
    "try:\n",
    "    print(\"‚è≥ Caricamento Test Set...\")\n",
    "    test_path = os.path.join(DATA_DIR, 'test_processed.pkl')\n",
    "    \n",
    "    df_test = pd.read_pickle(test_path)\n",
    "    \n",
    "    if len(df_test) > N_SAMPLES:\n",
    "        df_test_sample = df_test.sample(n=N_SAMPLES, random_state=42)\n",
    "        print(f\"‚ö†Ô∏è Uso un sample di {N_SAMPLES} righe (Totale: {len(df_test):,})\")\n",
    "    else:\n",
    "        df_test_sample = df_test\n",
    "        \n",
    "    le_path = os.path.join(DATA_DIR, 'label_encoder.pkl')\n",
    "    label_encoder = joblib.load(le_path)\n",
    "    class_names = label_encoder.classes_\n",
    "    print(f\"‚úÖ Classi caricate: {class_names}\")\n",
    "\n",
    "    y_test = df_test_sample['y_macro_encoded']\n",
    "    X_test = df_test_sample.drop(columns=['y_macro_encoded', 'y_specific'], errors='ignore')\n",
    "    feature_names = X_test.columns.tolist()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Errore caricamento dati: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2a756b",
   "metadata": {},
   "source": [
    "## 2. Caricamento Modelli Addestrati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5fdc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "# Cerca tutti i file .pkl nelle sottocartelle di models/\n",
    "model_files = glob.glob(os.path.join(MODELS_DIR, '*/*.pkl'))\n",
    "\n",
    "print(f\"üîç Trovati {len(model_files)} modelli:\")\n",
    "for p in model_files:\n",
    "    model_name = os.path.basename(p).replace('.pkl', '')\n",
    "    algo = os.path.basename(os.path.dirname(p))\n",
    "    display_name = f\"{algo} ({model_name})\"\n",
    "    print(f\"  - Carico: {display_name}...\")\n",
    "    try:\n",
    "        models[display_name] = joblib.load(p)\n",
    "    except Exception as e:\n",
    "        print(f\"    ‚ùå Errore: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab4d837",
   "metadata": {},
   "source": [
    "## 3. Confronto Metriche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad5a049",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for name, model in models.items():\n",
    "    print(f\"‚ö° Test {name}...\")\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    results.append({'Model': name, 'Accuracy': acc, 'F1-Score': f1})\n",
    "\n",
    "if results:\n",
    "    df_res = pd.DataFrame(results).sort_values('F1-Score', ascending=False)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.barplot(data=df_res, x='F1-Score', y='Model', palette='viridis')\n",
    "    plt.title('Classifica Modelli (F1-Score)')\n",
    "    plt.xlim(0.8, 1.0)\n",
    "    plt.show()\n",
    "    display(df_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93d9e8f",
   "metadata": {},
   "source": [
    "## 4. Matrici di Confusione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f207dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(f'Confusion Matrix: {name}')\n",
    "    plt.ylabel('Reale')\n",
    "    plt.xlabel('Predetto')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68d48d0",
   "metadata": {},
   "source": [
    "## 5. Explainable AI (SHAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d891a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "if results:\n",
    "    best_name = df_res.iloc[0]['Model']\n",
    "    best_model = models[best_name]\n",
    "    print(f\"üî¨ Analisi SHAP su: {best_name}\")\n",
    "    \n",
    "    try:\n",
    "        # Usa un subset piccolissimo per SHAP (√® lento)\n",
    "        X_shap = X_test.iloc[:100]\n",
    "        explainer = shap.TreeExplainer(best_model)\n",
    "        shap_values = explainer.shap_values(X_shap)\n",
    "        \n",
    "        # Per multiclasse, shap_values √® una lista. Prendiamo la classe DDoS (es. indice 2)\n",
    "        target_idx = 2 \n",
    "        if len(shap_values) > target_idx:\n",
    "            print(f\"üìä Spiegazione per classe: {class_names[target_idx]}\")\n",
    "            shap.summary_plot(shap_values[target_idx], X_shap)\n",
    "        else:\n",
    "            shap.summary_plot(shap_values, X_shap)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è SHAP non disponibile: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
